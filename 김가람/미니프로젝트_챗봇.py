# -*- coding: utf-8 -*-
"""미니프로젝트 챗봇.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ho8rmMtkz1_fQhCnjrC6sWjKdsA4l3lz

## 형태소 분석
"""

# konlpy 설치
!pip3 install konlpy

# Komoran을 이용한 자연어처리(형태소 분석)
from konlpy.tag import Komoran

# 코모란 형태소 분석기 객체 생성
komoran = Komoran()

text = "아버지가 방에 들어갑니다."

# 형태소 추출
morphs = komoran.morphs(text)
print(morphs)

# 형태소와 품사 태그 추출
pos = komoran.pos(text)
print(pos)

# 명사만 추출
nouns = komoran.nouns(text)
print(nouns)

"""### Komoran 품사 

|품사|설명|
|---|---|
|NNG|일반 명사|
|JKS|주격 조사|
|JKB|부사격 조사|
|VV|동사|
|EF|종결 어미|
|SF|마침표,물음표,느낌표|
"""

# 미등록 단어 형태소 분석
from konlpy.tag import Komoran

komoran = Komoran()
text = "우리 챗봇은 엔엘피를 좋아해"
pos = komoran.pos(text)
print(pos)

##[단어] [품사]
#엔엘피  NNG
#나는 내일, 어제의 너와 만난다 NNG
#시샵

# 위의 내용을 user_dic.tsv 파일로 저장한 후

# komora = Komoran(userdic='./user_dic.tsv')
# text = "우리 챗봇은 엔엘피를 좋아해"
# pos = komoran.pos(text)
# print(pos)

# 로 실행하면  엔엘피가 NNG로 표기됨

"""# **임베딩**
- 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정

## 단어 임베딩
- 원핫 인코딩
  - 요소들중 하나의 값만 1, 나머지 요솟값은 0인 인코딩
  - 희소벡터라고 한다
  - 단어 집합이라 불리는 사전을 먼저 만들어야하고
  - 말뭉치에 존재하는 모든 단어의 수가 차원을 결정(단어 100개 -> 100차원)
"""

from konlpy.tag import Komoran
import numpy as np

komoran = Komoran()
text = "오늘 날씨는 구름이 많아요"

# 명사만 추출
nouns = komoran.nouns(text)
print(nouns)

#단어 사전 구축 및 단어별 인덱스 부여
dics = {}
for word in nouns:
  if word not in dics.keys():
    dics[word] = len(dics)
print(dics)

# 원-핫 인코딩
nb_classes = len(dics)
targets = list(dics.values())
one_hot_targets = np.eye(nb_classes)[targets] 
# 넘파이의 eye()함수를 이용해 원핫벡터를 만들어줌
# [targets]를 이용해 생성된 단위행렬의 순서를 단어 사전의 순서에 맞게 정렬
print(one_hot_targets)

"""## 희소 표현과 분산 표현
- 표현하고자 하는 인덱스의 요소만 1이고 나머지는 0으로 표현되는 것이(원-핫잇코딩) 희소벡터 or 희소 행렬이라고 함 -> 희소표현
- 희소표현은 직관적이지만, 사전의 크기가 커질수록 메모리 낭비, 계산복잡도가 커지는 단점 존재
- 단어간의 연관성이 전혀 없어 의미를 담을 수 없음

<br>

## 분산표현 
- 이를 해결하기 위해 나온 방법이 **분산표현**
- 한 단어의 정보가 여러차원에 분산되어 표현
- 임베딩 기법에서 제일 많이 사용되는 방식
- 데이터 손실 최소화하면서 압축 가능
- 일반화 능력이 뛰어남(의미, 주변 단어 관계 등)

## Word2Vec 모델
- 신경망 기반 단어 임베딩의 대표적 방법
- 계산량을 획기적으로 줄여 빠른 학습이 가능
- CBOW / skip-gram 2가지 모델

### CBOW 모델
- 주변 단어들을 이용해 타깃단얼르 예측하는 신경망 모델
- 입력을 주변단어로 설정, 학습된 가중치 데이터를 임베딩 벡터로 활용
- 타깃 언어의 손실만 계산하면 되기 때문에 학습 속도가 빠름
- '윈도우' 범위를 통해 앞뒤로 몇 개의 단어까지 확인할지 결정이 가능

### skip-gram 모델
- CBOW 모델과 반대. 하나의 타깃언어를 이용해 주변 단어들을 예측하는 신경망 모델
- CBOW 모델에 비해 예측해야 하는 맥락이 많아짐
- 그만큼 분산 표현력이 우수해 CBOW 모델에 비해 임베딩 품질이 우수하다
"""

from gensim.models import Word2Vec
from konlpy.tag import Komoran
import time

# 네이버 영화 리뷰 데이터 읽어오기
def read_review_data(filename):
    with open(filename, 'r') as f:
        data = [line.split('\t') for line in f.read().splitlines()]
        data = data[1:]
    return data

# 학습 시간 측정 시작
start = time.time()

# 리뷰 파일 읽어오기
print('1) 말뭉치 데이터 읽기 시작')
review_data = read_review_data('/content/ratings.txt')
print(len(review_data)) # 리뷰 데이터 전체 개수
print('1) 말뭉치 데이터 읽기 완료 : ', time.time() - start)

# 문장 단위로 명사만 추출해 학습 입력 데이터로 만듦
print('2) 형태소에서 명사만 추출 시작')
komoran = Komoran()
docs = [komoran.nouns(sentence[1]) for sentence in review_data]
print('2) 형태소에서 명사만 추출 완료 :', time.time() - start)

#Word2Vec 모델 학습
print('3) Word2Vec 모델 학습 시작')
model = Word2Vec(sentences=docs, size=200, window=4, hs = 1, min_count=2, sg=1)
print('3) Word2Vec 모델 학습 완료 :', time.time() - start)

# 모델 저장
print('4) 학습된 모델 저장 시작')
model.save('nvmc.model')
print('4) 학습된 모델 저장 완료 :', time.time() - start)

#학습된 말뭉치 수, 코퍼스 내 전체 단어 수
print("corpus_count: ", model.corpus_count)
print("corpus_total_words: ", model.corpus_total_words)

# Word2Vec 모델 활용
from gensim.models import Word2Vec

# 모델 로딩
model = Word2Vec.load('nvmc.model')
print("corpus_total_words: ", model.corpus_total_words)

#'사랑'이라는 단어로 생성한 단어 임베딩 벡터
print('사랑: ', model.wv['사랑'])

#단어 유사도 계산
print("일요일 = 월요일\t", model.wv.similarity(w1='일요일', w2='월요일'))
print("안성기 = 배우\t", model.wv.similarity(w1='안성기', w2='배우'))
print("대기업 = 삼성\t", model.wv.similarity(w1='대기업', w2='삼성'))
print("일요일 = 삼성\t", model.wv.similarity(w1='일요일', w2='삼성'))
print("히어로 = 삼성\t", model.wv.similarity(w1='히어로', w2='삼성'))

# 가장 유사한 단어 추출
print(model.wv.most_similar("안성기", topn=5))
print(model.wv.most_similar("시리즈", topn=5))

"""## 챗봇 문답 데이터 감정 분류 모델 구현
- CNN 모델 구현, 이미지 뿐만 아니라 임베딩 품질만 괜찮다면 자연어 분류에서도 좋은 성능을 기대할 수 있음

- 사용할 데이터는 0,1,2:일상다반사, 이별(부정),사랑(긍정) 으로 나뉘는 감정 데이터임

"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import preprocessing #전처리 케라스 패키지
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate

# 데이터 읽어오기
train_file = "/content/ChatbotData.csv"
data = pd.read_csv(train_file, delimiter=',') # delimiter -> 구획을 나누는 구분자
features = data['Q'].tolist() # tolist()는 array 형태를 list 형태로 바꾸어줌
labels = data['label'].tolist()
# Q : 질문, label : 감정

#단어 인덱스 시퀀스 벡터
corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]
# text_to_word_sequence() 함수를 이용해 단어 시퀀스 생성
# 단어 시퀀스란 단어 토큰들의 순차적인 리스트 의미 
word_index = tokenizer.word_index
#-----------------------------------------------------------------------
tokenizer = preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(corpus)
sequences = tokenizer.texts_to_sequences(corpus)
# texts_to_sequences() 함수를 이용해 문장내 모든 단어를 시퀀스 번호로 변환
word_index = tokenizer.word_index
#-----------------------------------------------------------------------
MAX_SEQ_LEN = 15 #단어 시퀀스 벡터 크기
padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')
# 시퀀스 번호로 변환된 전체 벡터 크기를 동일하게 맞추기 위해 MAX_SEQ_LEN을 설정,
# MAX_SEQ_LEN보다 작은 크기의 벡터에는 남는 공간에 0으로 채우는데 이것을 'padding' 처리라고 부름
# keras에서는 pad_sequences()함수를 통해 처리할 수 있다.
#-----------------------------------------------------------------------

# 학습용, 검증용, 테스트용 데이터셋 생성
# 학습셋:검증셋:테스트셋 = 7 : 2: 1
ds = tf.data.Dataset.from_tensor_slices((padded_seqs,labels))
ds = ds.shuffle(len(features))

train_size = int(len(padded_seqs) * 0.7)
val_size = int(len(padded_seqs)*0.2)
test_size = int(len(padded_seqs)*0.1)

train_ds = ds.take(train_size).batch(20)
val_ds = ds.skip(train_size).take(val_size).batch(20)
test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)

#하이퍼 파라미터 설정
dropout_prob = 0.5
EMB_SIZE = 128
EPOCH = 5
VOCAB_SIZE = len(word_index) +1 #전체 단어 수
#-----------------------------------------------------------------------

# CNN 모델 정의
input_layer = Input(shape=(MAX_SEQ_LEN,))
embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)
dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)

conv1 = Conv1D(
    filters = 128,
    kernel_size = 3,
    padding='valid',
    activation=tf.nn.relu)(dropout_emb)
pool1 = GlobalMaxPool1D()(conv1)
# 임베딩 벡터를 합성곱 계층의 입력으로 받아 GlobalMaxPool1D()를 이용, 최대풀링 수행
# 이후 완전 연결 계층을 통해 클래스 분류작업
# concatenate()를 이용해 각각 병렬로 처리된 합성곱 계층의 특징맵 결과를 하나로 묶어줌
conv2 = Conv1D(
    filters = 128,
    kernel_size = 4,
    padding='valid',
    activation=tf.nn.relu)(dropout_emb)
pool2 = GlobalMaxPool1D()(conv2)

conv3 = Conv1D(
    filters = 128,
    kernel_size = 5,
    padding='valid',
    activation=tf.nn.relu)(dropout_emb)
pool3 = GlobalMaxPool1D()(conv3)

# 3, 4, 5-gram 이후 합치기
concat = concatenate([pool1,pool2,pool3])

#-----------------------------------------------------------------------

hidden = Dense(128, activation=tf.nn.relu)(concat)
#128개의 입력 노드를 가지고, relu 활성화 함수를 사용하는 Dense 계층 생성
dropout_hidden = Dropout(rate=dropout_prob)(hidden)
logits = Dense(3, name='logits')(dropout_hidden)
predictions = Dense(3, activation = tf.nn.softmax)(logits)
# Dense를 3으로 지정한 이유는 3가지 클래스로 감정 분류를 해야하기 때문(일상대화, 사랑, 슬픔 ....)
# logits : 점수
# logits에서 나온 점수를 소프트맥스 계층을 통해 감정 클래스 별 확률을 계산

# 클래스 분류 모델을 학습할때 주로 loss를 계산하는 함수로 'sparse_categorical_crossentropy'
# cross-entropy 계산을 위해 확률값을 입력으로 사용해야하는데, 그래서 소프트맥스 계층을 사용하는 것
#-----------------------------------------------------------------------

# 모델 생성
model = Model(inputs = input_layer, outputs=predictions)
model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 모델 학습
model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)
# train_ds : 학습용 데이터셋, validation_data : 검증용 데이터셋
# verbose :1 일경우, 모델 학습시 진행 과정 상세히 보여줌. 생략 원하면 0으로

# 모델 평가(테스트 데이터셋 이용)
loss, accuracy = model.evaluate(test_ds, verbose=1)
print('Accuracy: %f' % (accuracy * 100))
print('loss: %f' % (loss))

#모델 저장
model.save('cnn_model.h5')

