# -*- coding: utf-8 -*-
"""태현님 아마존 프로젝트.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ieagWF2srs8X30VpHSqJc_WR-RO5wjex

# 데이터 : 캐글 데이터/아마존 페이지 내 존재하는 도서 데이터 프레임
- 사용하는 변수는 책 제목, 이미지, 줄거리 입니다.

# 목표점
- 책 제목을 입력했을 때 해당하는 줄거리와 다른 도서들의 줄거리 간 유사도를 분석하여 가장 유사한 추천 도서 표지 출력하기

# 진행사항
- 데이터 전처리 (줄거리 변수의 값들에 대한 토큰화 진행)
- 보잘것 없는 Word2vec 분석 (모든 줄거리들 내에 존재하는 1개의 단어를 입력하면 유사한 단어들/유사도 출력)
- 도서 표지 이미지 출력 (그냥 구글링 코드 / 추후에는 추천 도서 5개를 출력하도록 하자.)
"""

import pandas as pd
import numpy as np
from string import punctuation
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
import nltk 
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.metrics.pairwise import cosine_similarity
import urllib

df = pd.read_csv('/content/drive/MyDrive/books_data.csv')
df = df[['Title', 'image','description']]                                     # 필요 변수만

df = df.dropna(axis = 0, how = 'any')                                         # 결측지 제거

# np.random.seed(123)
# label = np.random.randint(0,len(df),130000)                                     # 데이터 수가 너무 많아서 7000개 랜덤으로 추출
# df = df.iloc[label].reset_index()
# del df['index']

df.head(10)

df.describe() # count보다 unique가 적다면 중복값 존재함

df = df.drop_duplicates('description')    # 중복값 제거 데이터프레임 생성
df.index = range(len(df))    # 인덱스 재지정
df1 = df[:15000]

df_title = df.Title.copy()
df_des = df.description.copy()

"""# 데이터 전처리"""

def NLTK_des(data):
    
    data = data.str.lower()     # 소문자 변경
    
    nltk.download('punkt')
    nltk.download('stopwords')
    
    stw = stopwords.words('english')   # 불용어
    stw.extend(["'s", "’", "’", "''", "``", "--"])    # 불용어 + punctuation에 없는 특수문자들

    des_token = []

    for i in range(len(data)):
        result = []
        for w in word_tokenize(data[i]):
            if ((w not in stw) and (w not in punctuation)):
                result.append(w)
                tmp = " ".join(result)
        des_token.append(tmp)

    return pd.Series(des_token)


NLTK_des(df_des)

NLTK_des1 = NLTK_des(df_des).head(15000).copy()

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

tf_vec = TfidfVectorizer()

tf_vec_matrix = tf_vec.fit_transform(NLTK_des1)
print(tf_vec_matrix.shape)

# 유사도 형태
cosine_sim = cosine_similarity(tf_vec_matrix, tf_vec_matrix)
cosine_sim.shape

# 제목과 인덱스 번호 매칭
title_num = dict(zip(df1['Title'], df1.index))

# 인덱스 번호 매칭 테스트
title_num['Mensa Number Puzzles (Mensa Word Games for Kids)']

def get_recommand(title, sosine_sim = cosine_sim):
  idx = title_num[title]

  # 지정된 책의 유사도 산출
  sim_scores = list(enumerate(cosine_sim[idx]))

  # 유사도 순 정렬
  sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)
  #상위 10개 항목
  sim_scores = sim_scores[1:11]
  #가장 유사한 10개의 인덱스
  book_index = [idx[0] for idx in sim_scores]
  #상위 10개 항목 리턴
  print(sim_scores)
  return df1['Title'].iloc[book_index]

get_recommand('A husband for Kutani')

df['image'].head(20)

from PIL import Image
import requests
from io import BytesIO

response = requests.get(df1.iloc[0].image)
img = Image.open(BytesIO(response.content))
img

for i in range(10):
  response = requests.get(df1.iloc[i].image)
  img = Image.open(BytesIO(response.content))
  img

img

