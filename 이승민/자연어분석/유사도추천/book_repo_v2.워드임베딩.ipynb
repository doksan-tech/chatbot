{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드투벡터(Word2Vec)\n",
    "\n",
    "- 단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 할 수 있는 방법\n",
    "\n",
    "1. 희소 표현(Sparse Representation)\n",
    "- 대안: 분산 표현(distributed representation)\n",
    "\n",
    "2. 분산 표현(Distributed Representation)\n",
    "-  '비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다\n",
    "\n",
    "3. CBOW(Continuous Bag of Words)\n",
    "- Word2Vec의 학습 방식에는 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식\n",
    "- CBOW: 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법\n",
    "    - 윈도우(window): 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 결정해야 하는 범위 \n",
    "- Skip-Gram: 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 영어/한국어 Word2Vec 실습\n",
    "\n",
    "### 1.1.1 영어 Word2Vec 만들기\n",
    "\n",
    "- gensim 패키지에는 Word2Vec을 지원\n",
    "- 영어로 된 코퍼스를 다운받아 전처리를 수행하고, 전처리한 데이터를 바탕으로 Word2Vec 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.1.1 훈련 데이터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x1c23ba75b80>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 파일은 xml 문법으로 작성되어 있어 자연어를 얻기 위해서는 전처리가 필요\n",
    "# 실질적 데이터는 영어문장으로만 구성된 내용을 담고 있는 <content>와 </content> 사이의 내용\n",
    "# 전처리 작업을 통해 xml 문법들은 제거하고, 해당 데이터만 가져오기\n",
    "# <content>와 </content> 사이의 내용 중에는 (Laughter)나 (Applause)와 같은 배경음을 나타내는 단어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1.2 훈련 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('./data/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 273380\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# 샘플 3개만 출력\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1.3 Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec의 하이퍼파라미터\n",
    "\n",
    "size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "\n",
    "window = 컨텍스트 윈도우 크기\n",
    "\n",
    "min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "\n",
    "workers = 학습을 위한 프로세스 수\n",
    "\n",
    "sg = 0은 CBOW, 1은 Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8473332524299622), ('guy', 0.8179163336753845), ('lady', 0.7808512449264526), ('boy', 0.7582191228866577), ('soldier', 0.7409019470214844), ('girl', 0.7349250316619873), ('gentleman', 0.7311205267906189), ('kid', 0.7178890705108643), ('poet', 0.6997151374816895), ('rabbi', 0.663277804851532)]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec는 입력한 단어에 대해서 가장 유사한 단어들을 출력하는 model.wv.most_similar을 지원\n",
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1.4 Word2Vec 모델 저장하고 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('./data/eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"./data/eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8473332524299622), ('guy', 0.8179163336753845), ('lady', 0.7808512449264526), ('boy', 0.7582191228866577), ('soldier', 0.7409019470214844), ('girl', 0.7349250316619873), ('gentleman', 0.7311205267906189), ('kid', 0.7178890705108643), ('poet', 0.6997151374816895), ('rabbi', 0.663277804851532)]\n"
     ]
    }
   ],
   "source": [
    "# 로드한 모델에 대해서 다시 man과 유사한 단어를 출력\n",
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 한국어 Word2Vec 만들기(네이버 영화 리뷰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x1c27cd48be0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 네이버 영화리뷰 데이터 \n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화 리뷰 데이터를 데이터프레임으로 로드하고 상위 5개\n",
    "train_data = pd.read_table('./data/ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5] # 상위 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# 총 리뷰 개수를 확인\n",
    "print(len(train_data)) # 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 결측값 유무를 확인\n",
    "# NULL 값 존재 유무\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 결측값이 존재하므로 결측값이 존재하는 행을 제거\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199992\n"
     ]
    }
   ],
   "source": [
    "# 결측값이 삭제된 후의 리뷰 개수를 확인\n",
    "print(len(train_data)) # 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gnjnm\\AppData\\Local\\Temp\\ipykernel_27952\\3335061672.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    }
   ],
   "source": [
    "# 정규 표현식을 통해 한글이 아닌 경우 제거하는 전처리를 진행\n",
    "# 정규 표현식을 통한 한글 외 문자 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5] # 상위 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199992/199992 [1:29:46<00:00, 37.13it/s]    \n"
     ]
    }
   ],
   "source": [
    "# 불용어를 제거\n",
    "# 형태소 분석기 Okt를 사용하여 각 문장에 대해서 일종의 단어 내지는 형태소 단위로 나누는 토큰화를 수행\n",
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_data = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 72\n",
      "리뷰의 평균 길이 : 10.716703668146726\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토큰화가 된 상태에서는 각 리뷰의 길이 분포 또한 확인이 가능\n",
    "# 리뷰 길이 분포 확인\n",
    "print('리뷰의 최대 길이 :',max(len(review) for review in tokenized_data))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "plt.hist([len(review) for review in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec으로 토큰화 된 네이버 영화 리뷰 데이터를 학습\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)  # size 만 쓰면 Error\n",
    "model = Word2Vec(sentences = tokenized_data, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16477, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습이 다 되었다면 Word2Vec 임베딩 행렬의 크기를 확인\n",
    "# 완성된 임베딩 매트릭스의 크기 확인\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('최민수', 0.8640745282173157), ('안성기', 0.861038863658905), ('한석규', 0.8563510179519653), ('이민호', 0.8356214761734009), ('엄태웅', 0.8310987949371338), ('김수현', 0.8224194645881653), ('송강호', 0.8214665055274963), ('설경구', 0.8208147883415222), ('서영희', 0.8191223740577698), ('유다인', 0.8173700571060181)]\n",
      "[('슬래셔', 0.8928637504577637), ('호러', 0.8886445760726929), ('느와르', 0.8727526068687439), ('무협', 0.8708317279815674), ('블록버스터', 0.8705595135688782), ('멜로', 0.8312639594078064), ('무비', 0.8190279603004456), ('정통', 0.8154367208480835), ('헐리우드', 0.811850368976593), ('물', 0.8048973679542542)]\n"
     ]
    }
   ],
   "source": [
    "# 총 16,477개의 단어가 존재하며 각 단어는 100차원\n",
    "# '최민식'과 유사한 단어\n",
    "print(model.wv.most_similar(\"최민식\"))\n",
    "\n",
    "# '히어로'와 유사한 단어\n",
    "print(model.wv.most_similar(\"히어로\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1.  사전 훈련된 Word2Vec 임베딩(Pre-trained Word2Vec embedding) 소개\n",
    "\n",
    "- 감성 분류 작업을 하는데 훈련 데이터의 양이 부족한 상황\n",
    "- 다른 방대한 데이터를 Word2Vec이나 GloVe 등으로 사전에 학습시켜놓은 임베딩 벡터들을 가지고 와서 모델의 입력으로 사용하는 것\n",
    "- 사전 훈련된 워드 임베딩을 가져와서 간단히 단어들의 유사도를 구해보는 실습\n",
    "\n",
    "- 구글이 제공하는 사전 훈련된(미리 학습되어져 있는) Word2Vec 모델을 사용하는 방법\n",
    "- 구글은 사전 훈련된 3백만 개의 Word2Vec 단어 벡터들을 제공\n",
    "- 임베딩 벡터의 차원은 300\n",
    "- gensim을 통해서 이 모델을 불러오는 건 매우 간단합니다. 이 모델을 다운로드하고 파일 경로를 기재\n",
    "\n",
    "- 모델 다운로드 경로 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\chatbot_mini\\book_reco\\book_repo_v2.워드임베딩.ipynb 셀 37\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/chatbot_mini/book_reco/book_repo_v2.%EC%9B%8C%EB%93%9C%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/chatbot_mini/book_reco/book_repo_v2.%EC%9B%8C%EB%93%9C%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 구글의 사전 훈련된 Word2Vec 모델을 로드.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/chatbot_mini/book_reco/book_repo_v2.%EC%9B%8C%EB%93%9C%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlretrieve(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m, \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/chatbot_mini/book_reco/book_repo_v2.%EC%9B%8C%EB%93%9C%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                            filename\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGoogleNews-vectors-negative300.bin.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/chatbot_mini/book_reco/book_repo_v2.%EC%9B%8C%EB%93%9C%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m word2vec_model \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mKeyedVectors\u001b[39m.\u001b[39mload_word2vec_format(\u001b[39m'\u001b[39m\u001b[39mGoogleNews-vectors-negative300.bin.gz\u001b[39m\u001b[39m'\u001b[39m, binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot_mini/book_reco/book_repo_v2.%EC%9B%8C%EB%93%9C%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# 모델의 크기(shape)를 확인\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gnjnm\\anaconda3\\lib\\urllib\\request.py:239\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m url_type, path \u001b[39m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 239\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mclosing(urlopen(url, data)) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    240\u001b[0m     headers \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39minfo()\n\u001b[0;32m    242\u001b[0m     \u001b[39m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[39m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gnjnm\\anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\gnjnm\\anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[39mfor\u001b[39;00m processor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response\u001b[39m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[0;32m    525\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\gnjnm\\anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[39m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[39m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[0;32m    633\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[0;32m    635\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\gnjnm\\anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\gnjnm\\anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\gnjnm\\anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import urllib.request\n",
    "\n",
    "# 구글의 사전 훈련된 Word2Vec 모델을 로드.\n",
    "urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
    "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "\n",
    "# 모델의 크기(shape)를 확인\n",
    "print(word2vec_model.vectors.shape)\n",
    "\n",
    "# 모델의 크기는 3,000,000 x 300입니다. 즉, 3백만 개의 단어와 각 단어의 차원은 300\n",
    "# 사전 훈련된 임베딩을 사용하여 두 단어의 유사도를 계산\n",
    "print(word2vec_model.similarity('this', 'is'))\n",
    "print(word2vec_model.similarity('post', 'book'))\n",
    "\n",
    "# 단어 'book'의 벡터를 출력\n",
    "print(word2vec_model['book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. 네거티브 샘플링을 이용한 Word2Vec 구현\n",
    "(Skip-Gram with Negative Sampling, SGNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 20뉴스그룹 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 하나의 샘플에 최소 단어 2개 > 중심 단어, 주변 단어의 관계가 성립\n",
    "# 전처리 과정에서 지속적으로 이를 만족하지 않는 샘플들을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('총 샘플 수 :',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gnjnm\\AppData\\Local\\Temp\\ipykernel_27952\\669936866.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "# 전처리를 진행. 불필요한 토큰을 제거하고, 소문자화를 통해 정규화\n",
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 데이터프레임에 Null 값이 있는지 확인\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null 값이 없지만, 빈 값(empy) 유무도 확인\n",
    "# 모든 빈 값을 Null 값으로 변환하고, 다시 Null 값이 있는지 확인\n",
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10995\n"
     ]
    }
   ],
   "source": [
    "# Null 값이 있음을 확인했습니다. Null 값을 제거\n",
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수 :',len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK에서 정의한 불용어 리스트를 사용하여 불용어를 제거\n",
    "# 불용어를 제거\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnjnm\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    }
   ],
   "source": [
    "# 모든 샘플 중 단어가 1개 이하인 경우를 모두 찾아 제거\n",
    "# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\n",
    "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
    "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
    "print('총 샘플 수 :',len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  단어 집합을 생성하고, 정수 인코딩을 진행\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {value : key for key, value in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
     ]
    }
   ],
   "source": [
    "# 상위 2개의 샘플을 출력\n",
    "print(encoded[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 64277\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합의 크기를 확인\n",
    "vocab_size = len(word2idx) + 1 \n",
    "print('단어 집합의 크기 :', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 네거티브 샘플링을 통한 데이터셋 구성하기\n",
    "\n",
    "- 네거티브 샘플링을 통한 데이터셋을 구성할 차례\n",
    "- 네거티브 샘플링을 위해서 케라스에서 제공하는 전처리 도구인 skipgrams를 사용\n",
    "- 어떤 전처리가 수행되는지 그 결과를 확인하기 위해서 (꽤 시간이 소요되는 작업이므로) 상위 10개의 뉴스그룹 샘플에 대해서만 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "# 네거티브 샘플링\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rediculous (15227), dispersed (20444)) -> 0\n",
      "(degree (1530), austria (4866)) -> 1\n",
      "(ignore (1979), fnajr (22956)) -> 0\n",
      "(blessing (10669), commited (7837)) -> 1\n",
      "(israels (13686), refinements (62423)) -> 0\n"
     ]
    }
   ],
   "source": [
    "# 결과를 확인합니다. 10개의 뉴스그룹 샘플에 대해서 모두 수행되었지만, 첫번째 뉴스그룹 샘플에 대해서만 확인\n",
    "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          idx2word[pairs[i][0]], pairs[i][0], \n",
    "          idx2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 10\n"
     ]
    }
   ],
   "source": [
    "# 윈도우 크기 내에서 중심 단어, 주변 단어의 관계를 가지는 경우에는 1의 레이블\n",
    "# 그렇지 않은 경우는 0의 레이블\n",
    "# 각각의 뉴스그룹 샘플에 대해서 동일한 프로세스로 수행\n",
    "print('전체 샘플 수 :',len(skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 뉴스그룹 샘플이 가지고 있는 pairs와 labels의 개수를 출력\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 작업을 모든 뉴스그룹 샘플에 대해서 수행\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Skip-Gram with Negative Sampling(SGNS) 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터인 임베딩 벡터의 차원은 100으로 정하고, 두 개의 임베딩 층을 추가\n",
    "embedding_dim = 100\n",
    "\n",
    "# 중심 단어를 위한 임베딩 테이블\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
    "\n",
    "# 주변 단어를 위한 임베딩 테이블\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 임베딩 테이블은 중심 단어와 주변 단어 각각을 위한 임베딩 테이블\n",
    "# 각 단어는 임베딩 테이블을 거쳐서 내적을 수행\n",
    "# 내적의 결과는 1 또는 0을 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값\n",
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
    "output = Activation('sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "    print('Epoch :',epoch, 'Loss :',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 결과 확인하기\n",
    "\n",
    "- 학습된 모델의 결과를 확인\n",
    "- 학습된 임베딩 벡터들을 vector.txt에 저장\n",
    "- gensim의 models.KeyedVectors.load_word2vec_format()으로 로드하면 쉽게 단어 벡터 간 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "# 모델 로드\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['doctor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['police'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['knife'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['engine']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 글로브(GloVe)\n",
    "\n",
    "- 글로브(Global Vectors for Word Representation, GloVe)는 카운트 기반과 예측 기반을 모두 사용하는 방법론\n",
    "- 기존의 카운트 기반의 LSA(Latent Semantic Analysis)와 예측 기반의 Word2Vec의 단점을 지적하며 이를 보완한다는 목적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)\n",
    "\n",
    "- 단어의 동시 등장 행렬: 행과 열을 전체 단어 집합의 단어들로 구성하고, \n",
    "    - i 단어의 윈도우 크기(Window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 동시 등장 확률(Co-occurrence Probability)\n",
    "\n",
    "- 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 GloVe 훈련시키기\n",
    "\n",
    "- pip install glove_python_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "corpus = Corpus() \n",
    "\n",
    "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
    "corpus.fit(result, window=5)\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "\n",
    "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
    "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\gnjnm\\\\anaconda3\\\\python.exe'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-1. 패스트텍스트(FastText)\n",
    "\n",
    "- FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주\n",
    "- 내부 단어. 즉, 서브워드(subword)를 고려하여 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 내부 단어(subword)의 학습\n",
    "\n",
    "- FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급\n",
    "- n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정\n",
    "- 시작과 끝을 의미하는 <, >를 도입하여 아래의 5개 내부 단어(subword) 토큰을 벡터로 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 모르는 단어(Out Of Vocabulary, OOV)에 대한 대응\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 실습으로 비교하는 Word2Vec Vs. FastText\n",
    "\n",
    "- Word2Vec와 FastText의 차이를 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.4.1 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec의 실습( https://wikidocs.net/50739 )의 전처리 코드와 Word2Vec 학습 코드를 그대로 수행 완료 후\n",
    "# 입력 단어에 대해서 유사한 단어를 찾아내는 코드에 이번에는 electrofishing이라는 단어\n",
    "model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.4.2 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 학습 코드만 FastText 학습 코드로 변경하여 실행\n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrofishing에 대해서 유사 단어를 찾기\n",
    "model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 한국어에서의 FastText\n",
    "\n",
    "#### 6.1.5.1 음절 단위\n",
    "\n",
    "#### 6.1.5.2 자모 단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b6f0d8d647a14ce7aec9650edf6792f5fc61b159b6795cf61c93c7b5c41aef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
