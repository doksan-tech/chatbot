{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이턱 읽어오기 \n",
    "# pandas reda_csv() 함수 이용해 파일 읽어와 CNN 모델 학습 필요한 Q(질문), label(감정) 데이터를 features, labels 리스트 저장 \n",
    "train_file = './data/ChatbotData.csv'\n",
    "data = pd.read_csv(train_file, delimiter=',') \n",
    "features = data['Q'].tolist() \n",
    "labels = data['label'].tolist() \n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터 \n",
    "# 위에서 불러온 질문 리스트(features) 에서 문장 하나씩 꺼내와 text_to_word_sequence() 함수 이용해 단어 시퀀스 만듦\n",
    "# 단어시퀀스 = 단어 토큰들의 순차적 리스트 의미 \n",
    "# 단어 시퀀스를 말뭉치(corpus) 리스트에 저장\n",
    "# 텐서플로 토크나이저 texts_to_sequences() 함수 이용해 문장 내 모든 단어를 시퀀스 번호로 변환 \n",
    "# 변환된 시퀀스 번호 이용해 단어 임베딩 벡터 만들\n",
    "# 시퀀스 번호로 만든 벡터 문제: 문장 길이가 제각각 벡터 크기 다름 > CNN 모델 입력층 고정된 개수 입력 노드 가짐 \n",
    "# 시퀀스 번호로 변환된 전체 벡터 크기를 동일하게 맞춰줘야 함\n",
    "# MAX_SEQ_LEN = 15 보다 작은 벡터에는 남는 공간 생김 여길 0으로 채우는 작업 = 패딩 처리 \n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features] \n",
    "tokenizer = preprocessing.text.Tokenizer() \n",
    "tokenizer.fit_on_texts(corpus) \n",
    "sequences = tokenizer.texts_to_sequences(corpus) \n",
    "word_index = tokenizer.word_index \n",
    "\n",
    "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기 \n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post') \n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1 \n",
    "# 패딩처리된 시퀀스(padded_seq) 벡터 리스트와 감정 (labels) 리스트 전체를 데이터셋 객체로 만듦 \n",
    "# 데이터 랜덤 섞은 후 학습용, 검증용, 테스트용 데이터셋 7:2:1 나눠 실제 학습에 필요한 데이터셋 객체 분리 \n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels)) \n",
    "ds = ds.shuffle(len(features)) \n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7) \n",
    "val_size = int(len(padded_seqs) * 0.2) \n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20) \n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20) \n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)  \n",
    "\n",
    "# 하이퍼파라미터 설정 \n",
    "dropout_prob = 0.5 \n",
    "EMB_SIZE = 128 \n",
    "EPOCH = 5 \n",
    "VOCAB_SIZE = len(word_index) + 1   # 전체 단어 수 \n",
    "\n",
    "# CNN 모델 정의 \n",
    "# 케라스 함수형 모델 방식으로 구현 \n",
    "# 문장을 감정 클래스로 분류하는 CNN 모델은 전처리된 입력 데이터를 단어 임베팅 처리하는 영역 / 합성곱 필터와 연산을 통해 문장의 특징 정보(특징맵) 추출하고 평탄화 하는 영역 \n",
    "# 완전 연결 계층 통해 감정별로 클래스 분류하는 영역\n",
    "\n",
    "# 단어 임베딩 영역 코드 \n",
    "# 입력 계층은 케라스 Input() 함수 생성 - shape 인자로 입력 노드에 들어올 데이터 형상 지정 \n",
    "input_layer = Input(shape = (MAX_SEQ_LEN)) \n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer) # 임베딩 계층 \n",
    "dropout_emb = Dropout(rate = dropout_prob)(embedding_layer) # 50% 확률로 Dropout() 생성 > 오버피팅 대비\n",
    "\n",
    "# 임베딩 벡터에서 특징 추출하는 영역 구현\n",
    "conv1 = Conv1D(\n",
    "    filters = 128,\n",
    "    kernel_size = 3, \n",
    "    padding = 'valid',\n",
    "    activation = tf.nn.relu)(dropout_emb) \n",
    "pool1 = GlobalMaxPool1D()(conv1)    # 최대 풀링 연산 수행\n",
    "\n",
    "conv2 = Conv1D(\n",
    "    filters = 128,\n",
    "    kernel_size = 4, \n",
    "    padding = 'valid',\n",
    "    activation = tf.nn.relu)(dropout_emb) \n",
    "pool2 = GlobalMaxPool1D()(conv2) \n",
    "\n",
    "conv3 = Conv1D(\n",
    "    filters = 128,\n",
    "    kernel_size = 5, \n",
    "    padding = 'valid',\n",
    "    activation = tf.nn.relu)(dropout_emb) \n",
    "pool3 = GlobalMaxPool1D()(conv3) \n",
    "\n",
    "# 3, 4, 5 - gram 이후 합치기 \n",
    "concat = concatenate([pool1, pool2, pool3]) # 병렬로 처리된 합성곱 계층 특징맵 결과를 하나로 묶어줌 \n",
    "\n",
    "# 완전 연결 계층 구현\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)    # 128개 출력 노드, relu 활성화 함수 사용하는 Dense 계층 생성 3개의 특정맵 데이터 입력으로 ㅂ다음\n",
    "dropout_hidden = Dropout(rate = dropout_prob)(hidden) \n",
    "logits = Dense(3, name='logits')(dropout_hidden)      # 3가지 감정 분류. 출력노드 3개인 Dense() 생성. 활성화 함수 X logits = 점수\n",
    "predictions = Dense(3, activation=tf.nn.softmax)(logits)  # softmax = 확률계산 \n",
    "\n",
    "# 모델 생성 \n",
    "# 위에서 정의 한 계층들 케라스 모델에 추가하는 작업 \n",
    "model = Model(inputs=input_layer, outputs=predictions) \n",
    "model.compile(optimizer='adam',  # 최적화\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "# 모델 학습 \n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1) # (학습용, 검증용, 에포크, verbose = 1 이면 모델 학습 시 진행과정 보여줌)\n",
    "\n",
    "# 모델 평가 (테스트 데이터텟 이용) \n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1) \n",
    "print('Accuracy: %f' % (accuracy * 100)) \n",
    "print('loss: %f' % (loss)) \n",
    "\n",
    "# 모델 저장 \n",
    "model.save('cnn_model.h5') \n",
    "\n",
    "\n",
    "# evaluate() 함수 이용해 성능 평가 \n",
    "# 테스트용 데이터 셋 이용 \n",
    "loss, accuracy = model.evaluate(test_ds, verbose = 1) \n",
    "print('Accuracy: %f' % (accuracy * 100))\n",
    "print('loss : %f' % (loss)) \n",
    "\n",
    "# 학습 완료 h5 파일 포맷으로 저장\n",
    "model.save('cnn_model.h5') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca71aff40853dd846130b67975d07637ec6294bee047a07657cfc9075e3162be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
