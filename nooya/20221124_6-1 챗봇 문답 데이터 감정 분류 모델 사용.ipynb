{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 15, 128)      1715072     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 15, 128)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 13, 128)      49280       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 12, 128)      65664       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 11, 128)      82048       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 128)         0           ['conv1d[0][0]']                 \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 128)         0           ['conv1d_1[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 128)         0           ['conv1d_2[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 384)          0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_max_pooling1d_1[0][0]', \n",
      "                                                                  'global_max_pooling1d_2[0][0]'] \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          49280       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " logits (Dense)                 (None, 3)            387         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            12          ['logits[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,961,743\n",
      "Trainable params: 1,961,743\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "100/100 - 0s - loss: 0.0704 - accuracy: 0.9775 - 260ms/epoch - 3ms/step\n",
      "단어 시퀀스 : ['썸', '타는', '여자가', '남사친', '만나러', '간다는데', '뭐라', '해']\n",
      "단어 인덱스 시퀀스 : [   13    61   127  4320  1333 12162   856    31     0     0     0     0\n",
      "     0     0     0]\n",
      "문장 분류(정답) : 2\n",
      "감정 예측 점수 : [[1.4671178e-06 1.5193201e-07 9.9999833e-01]]\n",
      "감정 예측 클래스 : [2]\n"
     ]
    }
   ],
   "source": [
    "# 문장 감정 분류 CNN 모델 사용 \n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "from tensorflow.keras.models import Model, load_model \n",
    "from tensorflow.keras import preprocessing \n",
    "\n",
    "# 데이터 읽어오기 \n",
    "# pandas read_csv() 함수 이용. ChatbotData.csv 파일 읽어와 label(감정) 분류할 Q(질문) 데이터 features 리스트에 저장. \n",
    "# label 리스트는 CNN 모델이 예측한 분류 결과와 실제 분류값을 비교하기 위한 목적 사용\n",
    "train_file = './data/ChatbotData.csv' \n",
    "data = pd.read_csv(train_file, delimiter=',') \n",
    "features = data['Q'].tolist() \n",
    "labels = data['label'].tolist() \n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터 \n",
    "# 질문리스트 features 에서 한 문장씩 꺼내와 text_to_word_sequence() 함수 이용해 단어 시퀀스 만든 후 말뭉치(corpus) 리스트 저장 \n",
    "# tensorflow 토크나이저 texts_to_sequences() 함수 이용해 문장 내 모든 단어 시퀀스 번호로 변환 \n",
    "# 단어 시퀀스 벡터 크기 맞추기 위해 pad_sequences() 함수 사용하여 패딩 처리 \n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features] \n",
    "tokenizer = preprocessing.text.Tokenizer() \n",
    "tokenizer.fit_on_texts(corpus) \n",
    "sequences = tokenizer.texts_to_sequences(corpus) \n",
    "\n",
    "MAX_SEQ_LEN = 15   # 단어 시퀀스 벡터 크기 \n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding = 'post') \n",
    "\n",
    "# 테스트용 데이터셋 생성 \n",
    "# 위에서 처리한 시퀀스(padded_seqs) 벡터 리스트와 감정(labels) 리스트 전체를 데이터셋 객체로 만듦 \n",
    "# 데이터 랜덤으로 섞은 후 테스트용 데이터셋 2,000개 뽑아서 20개씩 처리 \n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels)) \n",
    "ds = ds.shuffle(len(features)) \n",
    "test_ds = ds.take(2000).batch(20)   # 테스트 데이터셋 \n",
    "\n",
    "# 감정 분류 CNN 모델 불러오기\n",
    "# 케라스 load_model() 함수 사용 모델 파일 불러오기 \n",
    "# 성공적 불러오면 학습 모델 객체 반환.\n",
    "# 파일 저장된 모델 정보 확인 summary() 함수 호출. 테스트셋 데이터 이용 모델 성틍 평가\n",
    "model = load_model('cnn_model.h5') \n",
    "model.summary() \n",
    "model.evaluate(test_ds, verbose = 2) \n",
    "\n",
    "#테스트용 데이터셋의 10212 번째 데이터 출력 \n",
    "# 망뭉치 데이터 리스트 10212번째 문장 감정 예측\n",
    "# 예측 앞서 데이터 확인\n",
    "print('단어 시퀀스 :', corpus[10212]) \n",
    "print('단어 인덱스 시퀀스 :', padded_seqs[10212]) \n",
    "print('문장 분류(정답) :', labels[10212]) \n",
    "\n",
    "# 테스트용 데이터셋의 10212번째 데이터 검증 예측 \n",
    "# 케라스 predict() 함수는 입력 데이터에 대해 각 클래스별로 예측한 점수를 반환\n",
    "# 텐서플로 argmax() 함수 이용해 분류 클래스들 중 예측 점수가 가장 큰 클래스 번호 계산 = 10212번째 문장이 어떤 감정 클래스에 포함되어 있는지 판단 \n",
    "picks = [10212] \n",
    "predict = model.predict(padded_seqs[picks]) \n",
    "predict_class = tf.math.argmax(predict, axis = 1) \n",
    "print('감정 예측 점수 :', predict) \n",
    "print('감정 예측 클래스 :', predict_class.numpy()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b6f0d8d647a14ce7aec9650edf6792f5fc61b159b6795cf61c93c7b5c41aef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
