{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 실습 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트 - gensim\n",
    "from gensim.models import Word2Vec \n",
    "from konlpy.tag import Komoran \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 말뭉치 데이터 읽기 시작\n",
      "200000\n",
      "1) 말뭉치 데이터 읽기 완료:  0.41110897064208984\n",
      "2) 형태소에만 명사만 추출 시작\n",
      "2) 형태소에서 명사만 추출 오나료 :  74.82887983322144\n",
      "3) Word2Vec 모델 학습 시작\n",
      "3) Word2Vec 모델 학습 완료:  88.49280905723572\n",
      "4) 학습된 모델 저장 시작\n",
      "4) 학습된 모델 저장 완료:  89.01395797729492\n",
      "corpus_count : 200000\n",
      "corpus_total_words : 1076899\n"
     ]
    }
   ],
   "source": [
    "# 네이버 영화 리뷰 데이터 읽어옴 \n",
    "# 네이버 영화리뷰 말뭉치 파일(ratings.txt) 읽어와 리스트 반환하는 함수\n",
    "# ratings.txt - 라인마다 id, document, label 칼럼 데이터 구분\n",
    "# read_review_data() 함순ㄴ 리뷰 데이터를 각 라인별로 읽어와 \\t 를 기준으로 데이터 분리. \n",
    "# 그 후 첫 번재 행의 헤더를 제거하고 리뷰 데이터만 반환\n",
    "def read_review_data(filename):\n",
    "    # with open(filename, 'r') as f:  Error txt 파일 다른이름으로 저장 에서 Ansi 로 저장(utf-8 > Ansi) \n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()] \n",
    "        data = data[1:]     # 헤더 제거\n",
    "    return data \n",
    "\n",
    "# 학습 시간 측정 시작\n",
    "# 각 코드 수행 속도 측정하기 위한 시작 시간 저장 \n",
    "start = time.time() \n",
    "\n",
    "# 리뷰파일 읽어오기 \n",
    "# read_review_data() 함수 호출해 현재 경로에 있는 ratings.txt 파일을 리스트 형태로 읽어옴\n",
    "print('1) 말뭉치 데이터 읽기 시작') \n",
    "review_data =  read_review_data('./data/ratings.txt') \n",
    "print(len(review_data))     # 리뷰 데이터 전체 개수\n",
    "print('1) 말뭉치 데이터 읽기 완료: ', time.time() - start) \n",
    "\n",
    "# 문장 단위로 명사만 추출해 학습 입력 데이터로 만듦 \n",
    "# Komoran 형태소 문석기를 이용해 불러온 리뷰 데이터에서 문장별로 명사만 추출 \n",
    "# sentence[1] 은 ratings.txt 파일에서 document 컬럼의 데이터 의미 \n",
    "print('2) 형태소에만 명사만 추출 시작') \n",
    "komoran = Komoran()  \n",
    "docs = [komoran.nouns(sentence[1]) for sentence in review_data] \n",
    "print('2) 형태소에서 명사만 추출 오나료 : ', time.time() - start) \n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "# 위에서 추출한 명사 리스트로 Word2Vec 모델 학습.\n",
    "# Word2Vec 주요 파라미터 \n",
    "# - sentences : Word2Vec 모델 학습에 필요한 문장 데이터. Word2Vec 모델의 입력값 사용 \n",
    "# - vector_size : 단어 임베딩 벡터 차원(크기) \n",
    "# - window: 주변 단어 윈도우 크기 \n",
    "# - hs : 0(0이 아닌 경우 음수 샘플링 사용), 1(모델 학습에 softmax 사용) \n",
    "# - min_count : 단어 빈도 최소 수 제한(설정된 min_count) 빈도 수 이하 단어들은 학습하지 않음) \n",
    "# - sg : 0(CBOW 모델), 1(skip-gram 모델) \n",
    "print('3) Word2Vec 모델 학습 시작') \n",
    "model = Word2Vec(sentences=docs, vector_size=200, window=4, hs=1, min_count=2, sg=1) \n",
    "print('3) Word2Vec 모델 학습 완료: ', time.time() - start) \n",
    "\n",
    "# 모델 저장\n",
    "# 현재 디렉터리에 nvmc.model 이름으로 저장 \n",
    "print('4) 학습된 모델 저장 시작') \n",
    "model.save('nvmc.model') \n",
    "print('4) 학습된 모델 저장 완료: ', time.time() - start) \n",
    "\n",
    "# 학습된 말뭉치 수, 코퍼스 내 전체 단어 수 \n",
    "print('corpus_count :', model.corpus_count) \n",
    "print('corpus_total_words :', model.corpus_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b6f0d8d647a14ce7aec9650edf6792f5fc61b159b6795cf61c93c7b5c41aef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
