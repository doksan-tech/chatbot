{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 프로젝트 구조 \n",
    "\n",
    "- 챗봇 프로젝트 디렉터리 구조\n",
    "\n",
    "- chatbot_test -- train_tools \n",
    "\n",
    "               - models --------- intent\n",
    "\n",
    "               - utils          ㄴ ner\n",
    "\n",
    "               - config\n",
    "           \n",
    "               - test\n",
    "\n",
    "- train_tools: 챗봇 학습툴 관련 파일 \n",
    "- models : 챗봇 엔진에서 사용하는 딥러닝 모델 관련 파일\n",
    "- intent : 의도 분류 모델 관련 파일 \n",
    "- ner : 개체 인식 모델 관련 파일 \n",
    "- utils : 챗봇 개발에 필요한 유틸리티 라이브러리 \n",
    "- config : 챗봇 개발에 필요한 설정 \n",
    "- test : 챗봇 개발에 필요한 테스트 코드   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-1. 학습용 데이터베이스 설계 및 데이터 테이블 생성 \n",
    "\n",
    "- 챗봇 엔진 답변 처리 과정에 필요한 DB 구조 설계\n",
    "- 학습할 수 있는 툴 만들기 \n",
    "\n",
    "- 학습툴에 필요한 DB 구조 설계 \n",
    "- 목표: 간단한 수준의 토이 챗봇 - 데이터 무결성, 정규화 큰 신경 X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 컬럼   | 속성                      | 설명                                                    |\n",
    "\n",
    " : ---     -------------------------    -----------------------------------------------------\n",
    "\n",
    "| id     | int primary key not null | 학습 데이터 id                                           |\n",
    "\n",
    "| intent | varchar(45)              | 의도명, 의도가 없는 경우 null                             |\n",
    "\n",
    "| ner    | varcahr(45)              | 개체명, 개체명이 없는 경우 null                           |\n",
    "\n",
    "| query  | text null                | 질문 텍스트                                              |\n",
    "\n",
    "| answer | text not null            | 답변 텍스트                                              |\n",
    "\n",
    "| answer | image varchar(2048)      | 답변에 들어갈 이미지 URL, 이미지 URL 사용하지 않을 경우 null|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2. DB 서버 접속 정보를 /config 디럭터리 내 파일로 따로 관리 \n",
    "\n",
    "- ./config/DatabaseConfig.py 파일 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-3. 챗봇 데이터 학습용 테이블 생성 코드 \n",
    "\n",
    "- 챗복 학습툴과 관련 \n",
    "- ./train_tools/qna/create_train_data_table.py 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'DB_PASSWORD' is not defined\n"
     ]
    }
   ],
   "source": [
    "# 경로 진행할 때 import sys 하고 진행해야 실행 됨 (다른 디렉터리 에서) \n",
    "\n",
    "import pymysql\n",
    "import sys\n",
    "from config.DBconfig import *  # DB 접속 정보 불러오기\n",
    "\n",
    "db = None\n",
    "try:\n",
    "    db = pymysql.connect(\n",
    "        host=DB_HOST,\n",
    "        user=DB_USER,\n",
    "        passwd=DB_PASSWORD,\n",
    "        db=DB_NAME,\n",
    "        charset='utf8'\n",
    "    )\n",
    "\n",
    "    # 테이블 생성 sql 정의\n",
    "    sql = '''\n",
    "      CREATE TABLE IF NOT EXISTS `chatbot_train_data` (\n",
    "      `id` INT UNSIGNED NOT NULL AUTO_INCREMENT,\n",
    "      `intent` VARCHAR(45) NULL,\n",
    "      `ner` VARCHAR(1024) NULL,\n",
    "      `query` TEXT NULL,\n",
    "      `answer` TEXT NOT NULL,\n",
    "      `answer_image` VARCHAR(2048) NULL,\n",
    "      PRIMARY KEY (`id`))\n",
    "    ENGINE = InnoDB DEFAULT CHARSET=utf8\n",
    "    '''\n",
    "\n",
    "    # 테이블 생성\n",
    "    with db.cursor() as cur:\n",
    "        cur.execute(sql)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    if db is not None:\n",
    "        db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. 챗봇 학습 데이터 엑셀 파일 및 DB 연동 \n",
    "\n",
    "- 지금 제작하는 학습툴은 화면이 없어서 엑셀 통해 학습 데이터 추가하거나 삭제\n",
    "- 엑셀 파일 학습툴에 입력해 DB 내용을 업데이트하는 형태\n",
    "- train_data.xlsx 구조\n",
    "    - 의도(intent): 질문 의도를 나타내는 텍스트. 의도가 없는 경우 비워둠\n",
    "    - 개체명 인식(NER): 질문에 필요한 개체명, 개체명이 없는 경우 비워둠\n",
    "    - 질문(query): 질문 테스트\n",
    "    - 답변(Answer): 답변 테스트\n",
    "    - 답변 이미지: 답변에 들어갈 이미지 URL. 이미지 URL이 없는 경우 비워둠\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'encoding'\n"
     ]
    }
   ],
   "source": [
    "# 엑셀파일 읽어와 DB와 데이터를 동기화하는 코드.\n",
    "# 챗봇 학습툴 디렉터리\n",
    "# 챗봇 학습 데이터 불러오기 - load_train_data.py 만들기 \n",
    "\n",
    "import pymysql\n",
    "import openpyxl \n",
    "import sys \n",
    "\n",
    "from config.DBconfig import *    # DB 접속 정보 불러오기 \n",
    "\n",
    "# 학습 데이터 초기화\n",
    "def all_clear_train_data(db): \n",
    "    # 기존 학습 데이터 삭제\n",
    "    sql = '''\n",
    "        delete from chatbot_train_data       \n",
    "    '''\n",
    "    with db.cursor() as cur: \n",
    "        cur.execute(sql) \n",
    "\n",
    "    # auto increament 초기화\n",
    "    sql = '''\n",
    "        ALTER TABLE chatbot_train_data AUTO_INCREMENT=1\n",
    "    '''\n",
    "    with db.cursor() as cur: \n",
    "        cur.execute(sql) \n",
    "\n",
    "# db에 데이터 저장 \n",
    "def insert_data(db, xls_row): \n",
    "    intent, ner, query, answer, answer_img_url = xls_row \n",
    "\n",
    "    sql = '''\n",
    "        INSERT chatbot_train_data(intent, ner, query, answer, answer_image)\n",
    "        values(\n",
    "            '%s','%s','%s','%s','%s'\n",
    "        )\n",
    "    ''' % (intent.value, ner.value, query.value, answer.value, answer_img_url.value) \n",
    "\n",
    "    # 엑셀에서 불러온 cell에 데이터가 없는 경우 null로 치환\n",
    "    sql = sql.replace(\"'None'\", \"null\") \n",
    "\n",
    "    with db.cursor() as cur:\n",
    "        cur.execute(sql) \n",
    "        print('{} 저장'.format(query.value)) \n",
    "        db.commit() \n",
    "\n",
    "train_file = './train_data.xlsx' \n",
    "db = None \n",
    "try:\n",
    "    db = pymysql.connect(\n",
    "        host = DB_HOST,\n",
    "        user = DB_USER,\n",
    "        passwd = DB_PASSWORD,\n",
    "        db = DB_NAME, \n",
    "        charset='utf8'\n",
    "    )\n",
    "\n",
    "    # 기존 학습 데이터 초기화 \n",
    "    # load_train_data.py 프로그램 실행할 때마다 엑셀 파일 내부 데이터와 DB 내 학습 데이터를 동일하게 \n",
    "    # 유지하기 위해 DB 데이터를 초기화 함.\n",
    "    # 이 경우 매번 DB 데이터를 지우고 새로 데이터를 입력하는 구조여서 추후 개선 필요 \n",
    "    # delete 명령어 사용해 챗봇 학습 데이터 테이블 내용 삭제한 후 auto increment 속성 1로 초기화 (AUTO INCREMENT=1) \n",
    "    all_clear_train_data(db) \n",
    "\n",
    "    # 학습 엑셀 파일 불러오기 \n",
    "    # openpyxl 모듈 이용해 엑셀 파일 읽어와 DB에 데이터 저장 \n",
    "    wb = openpyxl.load_workbook(train_file) \n",
    "    sheet = wb['Sheet1']\n",
    "    for row in sheet.iter_rows(min_row=2):    # 헤더는 불러오지 않음 \n",
    "        # 데이터 저장 \n",
    "        insert_data(db, row) \n",
    "\n",
    "    wb.close() \n",
    "\n",
    "except Exception as e: \n",
    "    print(e) \n",
    "\n",
    "finally:\n",
    "    if db is not None: \n",
    "        db.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 챗봇 엔진 만들기\n",
    "\n",
    "## 8.1 챗봇 엔진 \n",
    "\n",
    "- 챗봇에서 핵심 기능을 하는 모듈\n",
    "- 화자 질문 이해하고 알맞은 답변을 출력하는 역할 = 자연어 처리 모듈 \n",
    "- 카카오톡, 네이버톡톡 봇 빌더 \n",
    "\n",
    "## 8.2 챗봇 엔진 구조 \n",
    "\n",
    "- 엔진 설계 전에 만들려는 챗복의 목적과 어떤 도메인 지식을 가지는 챗봇을 만들 것인지 결정 \n",
    "    - 챗봇 엔진 개발 방법론, 학습에 필요한 데이터셋이 달라짐 \n",
    "    - 음식 예약 및 주문을 도와주는 음식점 예약 주문에 특화된 챗봇 엔진 실습\n",
    "\n",
    "- 토이 수준 챗봇 엔진 기능 = 5가지 \n",
    "    - 질문 의도 분류: 화자 질문 의도 파악. 의도 분류 모델 이요해 의도 클래스 예측 \n",
    "    - 개체명 인식: 화자 질문에서 단어 토큰별 개체명 인식. 단어 토큰에 맞는 개체명 예측하는 문제\n",
    "    - 핵심 키워드 추출: 화자 질문 의미에서 핵심이 될 만한 단어 토큰 추출. 형태소 분석기 이용 핵심 키워드 되는 명사나 동사 추출\n",
    "    - 답변 검색: 해당 질문 의도. 개체명, 핵심 키워드 등을 기반으로 답변을 학습 DB 에서 검색 \n",
    "    - 소켓 서버: 다양한 종류(카카오톡, 네이버톡톡) 챗봇 클라이언트에서 요청하는 질문 처리 위해 소켓 서버 프로그램 역할. = 챗봇 엔진 서버 프로그램 \n",
    "\n",
    "- 챗봇 엔진 처리 과정\n",
    "    1. 질문 문장 입력 - 챗봇엔진 = 전처리 > 형태소 분석기 이용 단어 토큰(키워드) 추출하고 명사나 동사 등 필요 품사만 남기고 불용어 제거 \n",
    "    2. 의도 분석과 개체명 인식 완료\n",
    "    3. 결괏값 이용 적적한 답변 학습 DB 에서 검색\n",
    "    4. 화자에게 답변 출력 \n",
    "\n",
    "- 자연어 처리 위한 2가지 딥러닝 모델(의도분석, 개체명 인식) \n",
    "- 도메인 지식에 맞는 딥러닝 모델 학습 데이터셋을 많이 보유하면 성능 우수한 챗봇 엔진 개발 도움 \n",
    "- 룰 베이스 + 딥러닝 모델 같이 사용하는 챗봇 엔진 \n",
    "\n",
    "## 8.3 전처리 과정 \n",
    "\n",
    "- 형태소 분석기로 토크나이징 작업 하고, 문장 해석에 의미 있는 정보만 남기고 나머지 불용어 들은 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 엔진 전처리 모듈(클래스) 만들기\n",
    "# preprocess.py\n",
    "# 클래스로 정의 - 자주 사용 \n",
    "\n",
    "from konlpy.tag import Komoran \n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, userdic=None):    # 생성자 \n",
    "        # 형태소 분석기 초기화 \n",
    "        # Preprocess 클래스 생성될 때 형태소 분석기 인스턴스 객체 생성.\n",
    "        # 형태소 분석기 = komoran \n",
    "        # userdic 인자에 사용자 정의 사전 파일 경로 입력 가능 \n",
    "        self.komoran = Komoran(userdic=userdic) \n",
    "\n",
    "        # 제외할 품사(불용어 정의) \n",
    "        # 참조: https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거 \n",
    "        # 어미 제거 \n",
    "        # 접미사 제거 \n",
    "        # 클래스 멤버 변수 exclusion.tags 리스트에 정의 \n",
    "        self.exclusion_tags = [\n",
    "            'JKS','JKC','JKG','JKO','JKB','JKV','JKQ','JX','JC',\n",
    "            'SF','SP','SS','SE','SO',\n",
    "            'EP','EF','EC','ETN','ETM',\n",
    "            'XSN','XSV','XSA'\n",
    "        ]\n",
    "\n",
    "    # 형태소 분석기 POS 태거 \n",
    "    # Preprocess 클래스 외부에서 코모란 형태소 분석기 객체를 직접 호출할 일이 없게 하기 위해 정의한 래퍼 함수 \n",
    "    # 형태소 분석기 종류를 바꾸게 될 경우 이 래퍼 함수 내용만 변경하면 됨 = 유지보수 장점 \n",
    "    def pos(self, sentence):\n",
    "        return self.komoran.pos(sentence) \n",
    "\n",
    "    # 불용어 제거 후 필요한 핵심 키워드(품사) 정보만 가져오기 \n",
    "    # exclusion_tags 리스트에 해당하지 않는 품사 정보만 키워드로 저장 \n",
    "    def get_keywords(self, pos, without_tag = False): \n",
    "        f = lambda x: x in self.exclusion_tags \n",
    "        word_list = [] \n",
    "        for p in pos: \n",
    "            if f(p[1]) is False: \n",
    "                word_list.append(p if without_tag is False else p[0]) \n",
    "        return word_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('내일', 'NNG'), ('오전', 'NNP'), ('10', 'SN'), ('시', 'NNB'), ('탕수육', 'NNP'), ('먹', 'VV'), ('싶', 'VX')]\n",
      "['내일', '오전', '10', '시', '탕수육', '먹', '싶']\n"
     ]
    }
   ],
   "source": [
    "# /test.preprocess_test.py 파일 생성\n",
    "# preprocess 클래스 동작 테스트 \n",
    "\n",
    "from utils.preprocess import *\n",
    "\n",
    "sent = '내일 오전 10시에 탕수육 먹고 싶어' \n",
    "\n",
    "# 전처리 객체 생성 \n",
    "p = Preprocess(userdic='../utils/user_dic.tsv')\n",
    "\n",
    "# 형태소 분석기 실행 \n",
    "pos = p.pos(sent) \n",
    "\n",
    "# 품사 태그와 같이 키워드 출력 \n",
    "ret = p.get_keywords(pos, without_tag=False) \n",
    "print(ret) \n",
    "\n",
    "# 품사 태그 없이 키워드 출력\n",
    "ret = p.get_keywords(pos, without_tag=True) \n",
    "print(ret) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 단어 사전 구축 및 시퀀스 생성 \n",
    "\n",
    "- 의도 분류 및 개체명 인식 모델의 학습을 하려면 단어 사전을 구축해야 함\n",
    "- 말뭉치 데이터(corpus.txt) /train_tools/dict 디렉터리 \n",
    "- create_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전 생성\n",
    "# 챗봇에서 사용하는 사전 파일 생성 \n",
    "\n",
    "from utils.preprocess import Preprocess \n",
    "from tensorflow.keras import preprocessing \n",
    "import pickle \n",
    "\n",
    "# 말뭉치 데이터 읽어오기 \n",
    "def read_corpus_data(filename): \n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()] \n",
    "        data = data[1:]   # 헤더 제거 \n",
    "    return data \n",
    "\n",
    "# 말뭉치 데이터 가져오기 \n",
    "# 말뭉치 파일 가져와 리스트로 반환\n",
    "# corpus.txt = 네이버 영화 리뷰 말뭉치 데이터 기반 데이터 \n",
    "# 라인마다 tab (\\t) 기준 데이터 분리\n",
    "corpus_data = read_corpus_data('./train_tools/dict/corpus.txt')\n",
    "\n",
    "# 말뭉치 데이터에서 키워드만 추출해서 사전 리스트 생성 \n",
    "# 문장 하나씩 불러와 POS 태깅\n",
    "# 형태소 분석 결과를 단어 리스트(dict) 에 저장 \n",
    "p = Preprocess() \n",
    "dict = [] \n",
    "for c in corpus_data: \n",
    "    pos = p.pos(c[1]) \n",
    "    for k in pos: \n",
    "        dict.append(k[0]) \n",
    "\n",
    "# 사전에 사용될 word2index 생성 \n",
    "# 사전의 첫 번째 인덱스에는 OOV 사용 \n",
    "tokenizer = preprocessing.text.Tokenizer(oov_token='OOV') \n",
    "tokenizer.fit_on_texts(dict) \n",
    "word_index = tokenizer.word_index \n",
    "\n",
    "# 사전 파일 새성\n",
    "f = open(\"chatbot_dic.bin\", \"wb\") \n",
    "try:\n",
    "    pickle.dump(word_index, f) \n",
    "except Exception as e:\n",
    "    print(e) \n",
    "\n",
    "finally:\n",
    "    f.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내일 14\n",
      "오전 269\n",
      "10시 1\n",
      "짜장면 527\n",
      "먹 233\n",
      "싶 11\n",
      "ㅋㅋ 10728\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 테스트 코드 \n",
    "\n",
    "import pickle \n",
    "from utils.preprocess import Preprocess \n",
    "\n",
    "# 단어 사전 불러오기 \n",
    "f = open(\"./train_tools/dict/chatbot_dic.bin\", \"rb\") \n",
    "word_index = pickle.load(f) \n",
    "f.close() \n",
    "\n",
    "sent = \"내일 오전 10시에 짜장면 먹고 싶어 ㅋㅋ\" \n",
    "\n",
    "# 전처리 객체 생성 \n",
    "p = Preprocess(userdic='./utils/user_dic.tsv') \n",
    "\n",
    "# 형태소 분석기 실행 \n",
    "pos = p.pos(sent) \n",
    "\n",
    "# 품사 태그 없이 키워드 출력 \n",
    "keywords = p.get_keywords(pos, without_tag=True) \n",
    "for word in keywords:\n",
    "    try:\n",
    "        print(word, word_index[word]) \n",
    "\n",
    "    except KeyError:\n",
    "        # 해당 단어가 사전에 없는 경우 OOV 처리 \n",
    "        print(word, word_index['OOV']) \n",
    "\n",
    "# 단어 시퀀스 벡터 크기 \n",
    "MAX_SEQ_LEN = 15 \n",
    "\n",
    "def GlobalParams(): \n",
    "    global MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력한 문장 단어 인덱스 사전 이용해 단어 시퀀스 벡터로 변환하는 기능 추가 \n",
    "# 챗봇 엔진 전처리 과정 포함 \n",
    "# Preprocess.py 클래스의 메서드 \n",
    "from konlpy.tag import Komoran\n",
    "import pickle\n",
    "import jpype\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, word2index_dic='', userdic=None):\n",
    "        # 단어 인덱스 사전 불러오기\n",
    "        if(word2index_dic != ''):\n",
    "            f = open(word2index_dic, \"rb\")\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            self.word_index = None\n",
    "\n",
    "        # 형태소 분석기 초기화\n",
    "        self.komoran = Komoran(userdic=userdic)\n",
    "\n",
    "        # 제외할 품사\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거\n",
    "        # 어미 제거\n",
    "        # 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC',\n",
    "            'SF', 'SP', 'SS', 'SE', 'SO',\n",
    "            'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "            'XSN', 'XSV', 'XSA'\n",
    "        ]\n",
    "\n",
    "    # 형태소 분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        jpype.attachThreadToJVM()\n",
    "        return self.komoran.pos(sentence)\n",
    "\n",
    "    # 불용어 제거 후, 필요한 품사 정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list\n",
    "\n",
    "    # 키워드를 단어 인덱스 시퀀스로 변환\n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "\n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                # 해당 단어가 사전에 없는 경우, OOV 처리\n",
    "                w2i.append(self.word_index['OOV'])\n",
    "        return w2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 의도 분류 모델 \n",
    "\n",
    "- 챗봇 엔진에 화자 질의 입력 되면 전처리 과정 거치고 해당 문장 의도 분류 \n",
    "- 문장을 의도 클래스 별로 분류하기 위해 CNN 모델 사용\n",
    "- 인사, 욕설, 주문, 예약, 기타 \n",
    "\n",
    "- GlobalParams.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlobalParams.py \n",
    "# 단어 시퀀스 벡터 크기 \n",
    "MAX_SEQ_LEN = 15 \n",
    "\n",
    "def GlobalParams():\n",
    "    global MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1 의도 분류 모델 학습 \n",
    "\n",
    "- total_train_data.csv  학습 데이터셋 \n",
    "- 음식점 주문과 예약을 위한 챗봇 특화 \n",
    "- 의도 분류 모델 생성 및 학습 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105658, 15)\n",
      "105658\n",
      "Epoch 1/5\n",
      "3698/3698 [==============================] - 69s 18ms/step - loss: 0.0552 - accuracy: 0.9824 - val_loss: 0.0128 - val_accuracy: 0.9960\n",
      "Epoch 2/5\n",
      "3698/3698 [==============================] - 69s 19ms/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 0.0086 - val_accuracy: 0.9959\n",
      "Epoch 3/5\n",
      "3698/3698 [==============================] - 66s 18ms/step - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.0079 - val_accuracy: 0.9971\n",
      "Epoch 4/5\n",
      "3698/3698 [==============================] - 70s 19ms/step - loss: 0.0098 - accuracy: 0.9962 - val_loss: 0.0064 - val_accuracy: 0.9972\n",
      "Epoch 5/5\n",
      "3698/3698 [==============================] - 67s 18ms/step - loss: 0.0085 - accuracy: 0.9966 - val_loss: 0.0054 - val_accuracy: 0.9975\n",
      "529/529 [==============================] - 1s 1ms/step - loss: 0.0071 - accuracy: 0.9975\n",
      "Accuracy: 99.753904\n",
      "loss: 0.007075\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 임포트\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "\n",
    "# 데이터 읽어오기\n",
    "# CNN모델 학습 위해 query, intent 데이터를 quries 와 intents 리스트에 저장 \n",
    "# 챗본 전처리 모듈 preprocess1 단어 시퀀스 생성 - 매치오디는 번호로 시퀀스 생성\n",
    "train_file = \"./models/intent/total_train_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()\n",
    "\n",
    "from utils.preprocess1 import Preprocess\n",
    "# p = Preprocess(word2index_dic='../../train_tools/dict/chatbot_dict.bin',\n",
    "#                userdic='../../utils/user_dic.tsv')\n",
    "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dic.bin',\n",
    "               userdic='./utils/user_dic.tsv')\n",
    "\n",
    "\n",
    "# 단어 시퀀스 생성\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag=True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)\n",
    "\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터 생성\n",
    "# 단어 시퀀스 벡터 크기\n",
    "# 단어 시퀀스 벡터 크기 동일하게 맞추기 위해 MAX_SEQ_LEN 크기만큰 시퀀스 벡터 패딩 처리 \n",
    "from config.GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "# (105658, 15)\n",
    "print(padded_seqs.shape)\n",
    "print(len(intents)) #105658\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성 \n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "# 패딩 처리된 시퀀ㅅ(padded_seqs) 벡터 리스트와 의도(intent) 리스트 전체 데이터셋 객체로 만듦 \n",
    "# 데이터 랜덤으로 섞고 학습용, 검증용, 테스트용 나누고 실제 학습에 필요한 데이터셋 객체 각각 분리 \n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.word_index) + 1 #전체 단어 개수\n",
    "\n",
    "\n",
    "# CNN 모델 정의  \n",
    "# 의도분류 모델은 케라스 함수형 모델 방식으로 구현\n",
    "# 입력하는 문장을 의도 클래스로 분류하는 CNN모델은 \n",
    "# 전처리된 입력 데이터를 단어 임베딩 처리하는 영역,\n",
    "# 합성곱 필터와 연산을 통해 문장의 특징 정보(특징맵) 를 추출하고 평탄화 하는 영역,\n",
    "# 완전 연결 계층(fully connected layer) 통해 감정별로 클래스 분류하는 영역\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=4,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=5,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# 3,4,5gram 이후 합치기\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "# 5가지 의도 클래스 분류해야 해서 출력 노드가 5개인 Dense 계층 생성\n",
    "# 신경망 예측 최종단계라 활성화 함수 사용 안함\n",
    "# 결과로 나온 값 logits = 점수\n",
    "# 출력 노드에서 5개 점수 출력 가장 큰 점수 가진 노드 위치가 CNN 모델이 예측한 의도 클래스가 됨\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(5, name='logits')(dropout_hidden)\n",
    "# 마지막 출력노드로 정의한 logits 에서 나온 점수를 소프트맥스 계츧을 통해 감정 클래스별 확률 계산 \n",
    "predictions = Dense(5, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "\n",
    "# 모델 생성 \n",
    "# 위에서 정의한 계층들을 케라스 모델에 추가하는 작업\n",
    "# Model 인자(입력계층, 출력계층) 사용. \n",
    "# 실제 모델을 model.compile() 함수 통해 CNN 모델 컴파일 \n",
    "# 최적화 방법 = 'adam', 손실 함수 = sparse_categorical_crossentropy   \n",
    "# 모델 평가 = 정확도 accoracy  \n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# CNN 모델 학습 \n",
    "# 첫번인자 = 학습용 데이터셋, 두번째 validation_data = 검증용 데이터셋, 에포크=5\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "\n",
    "# 모델 성능 평가(테스트 데이터 셋 이용)\n",
    "# 인자: 테스트용 데이터셋 \n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))\n",
    "print('loss: %f' % (loss))\n",
    "\n",
    "\n",
    "# 모델 저장  ○8\n",
    "model.save('intent_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.2 의도 분류 모듈 생성 \n",
    "\n",
    "- 챗봇 엔진 의도 분류 모듈 만들기\n",
    "- 학습한 의도 분류 모델 파일을 활용해 입력되는 텍스트의 의도 클래스를 예측하는 기능 \n",
    "- /models/intent.IntentModel.py 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 챗봇 엔진 의도 분류 모델 모듈\n",
    "# import tensorflow as tf \n",
    "# from tensorflow.keras.models import Model, load_model \n",
    "# from tensorflow.keras import preprocessing \n",
    "\n",
    "# # 의도 분류 모델 모듈 \n",
    "# class IntentModel: \n",
    "#     def __init__(self, model_name, preprocess): \n",
    "#         # 의도 클래스별 레이블 \n",
    "#         self.labels = {0: \"인사\", 1: \"욕설\", 2: \"주문\", 3: \"예약\", 4: \"기타\"}\n",
    "\n",
    "#         # 의도 분류 모델 불러오기 \n",
    "#         self.model = load_model(model_name) \n",
    "\n",
    "#         # 챗봇 Preprocess 객체 \n",
    "#         self.p = preprocess \n",
    "\n",
    "#     # 의도 클래스 예측 \n",
    "#     def predict_class(self, query): \n",
    "#         # 형태소 분석 \n",
    "#         pos = self.p.pos(query) \n",
    "\n",
    "#         # 문장 내 키워드 추출(불용어 제거) \n",
    "#         keywords = self.p.get_keywords(pos, without_tag=True) \n",
    "#         sequences = [self.p.get_wordidx_sequence(keywords)] \n",
    "\n",
    "#         # 단어 시퀀스 벡터 크기 \n",
    "#         from config.GlobalParams import MAX_SEQ_LEN \n",
    "\n",
    "#         # 패딩 처리 \n",
    "#         padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post') \n",
    "        \n",
    "#         predict = self.model.predict(padded_seqs) \n",
    "#         predict_class = tf.math.argmax(predict, axis = 1) \n",
    "#         return predict_class.numpy()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IntentModel' object has no attribute 'model_predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\chatbot_test\\20221124_01 chatbot_1_프로젝트실습.ipynb 셀 22\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot_test/20221124_01%20chatbot_1_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EC%8B%A4%EC%8A%B5.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m intent \u001b[39m=\u001b[39m IntentModel(model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./models/intent/intent_model.h5\u001b[39m\u001b[39m'\u001b[39m, preprocess\u001b[39m=\u001b[39mp) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot_test/20221124_01%20chatbot_1_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EC%8B%A4%EC%8A%B5.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m오늘 육개장 주문 가능한가요?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/chatbot_test/20221124_01%20chatbot_1_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EC%8B%A4%EC%8A%B5.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m predict \u001b[39m=\u001b[39m intent\u001b[39m.\u001b[39;49mpredict_class(query) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot_test/20221124_01%20chatbot_1_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EC%8B%A4%EC%8A%B5.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m predict_label \u001b[39m=\u001b[39m intent\u001b[39m.\u001b[39mlabels[predict] \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/chatbot_test/20221124_01%20chatbot_1_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EC%8B%A4%EC%8A%B5.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(query) \n",
      "File \u001b[1;32mc:\\chatbot_test\\models\\intent\\IntentModel.py:33\u001b[0m, in \u001b[0;36mIntentModel.predict_class\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m# 패딩 처리 \u001b[39;00m\n\u001b[0;32m     31\u001b[0m padded_seqs \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39msequence\u001b[39m.\u001b[39mpad_sequences(sequences, maxlen\u001b[39m=\u001b[39mMAX_SEQ_LEN, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[1;32m---> 33\u001b[0m predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_predict(padded_seqs) \n\u001b[0;32m     34\u001b[0m predict_class \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39margmax(predict, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m predict_class\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'IntentModel' object has no attribute 'model_predict'"
     ]
    }
   ],
   "source": [
    "# IntentModel 클래스 테스트 코드 \n",
    "# IntentModel 객체 생성해 새로운 유형의 문장 분류 \n",
    "\n",
    "from utils.preprocess1 import Preprocess \n",
    "from models.intent.IntentModel import IntentModel \n",
    "\n",
    "# p = Preprocess(word2index_dic='../train_tools/dict/chatbo_dic.bin',\n",
    "#                userdic = '../utils/user_dic.tsv') \n",
    "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dic.bin',\n",
    "               userdic = './utils/user_dic.tsv') \n",
    "\n",
    "# intent = IntentModel(model_name = '../models/intent/intent_model.h5', preprocess=p)\n",
    "intent = IntentModel(model_name = './models/intent/intent_model.h5', preprocess=p) \n",
    "query = '오늘 육개장 주문 가능한가요?'\n",
    "predict = intent.predict_class(query) \n",
    "predict_label = intent.labels[predict] \n",
    "\n",
    "print(query) \n",
    "print('의도 예측 클래스 : ', predict) \n",
    "print('의도 예측 레이블 : ', predict_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 개체명 인식 모델 학습 \n",
    "\n",
    "- 챗봇 엔진에 입력된 문장의 의도가 분류된 후 문장 내 개체명 인식을 진행 \n",
    "- 개체명 인식 LSTM 모델 사용 \n",
    "- 주요 개체명\n",
    "    - B_FOOD: 음식\n",
    "    - B_DT.B_TI: 날짜.시간\n",
    "    - B_PS: 사람 \n",
    "    - B_OG: 조직, 회사 \n",
    "    - B_LC: 지역 \n",
    "\n",
    "### 8.6.1 개체명 인식 모델 학습 \n",
    "\n",
    "- ner_trian.txt = 학습 데이터셋 \n",
    "- NER 모델 생성하고 학습하는 코드 \n",
    "- train_model.py 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 크기 : \n",
      " 61999\n",
      "0번 째 샘플 단어 시퀀스 : \n",
      " ['가락지빵', '주문', '하', '고', '싶', '어요']\n",
      "0번 째 샘플 bio 태그 : \n",
      " ['B_FOOD', 'O', 'O', 'O', 'O', 'O']\n",
      "샘플 단어 시퀀스 최대 길이 : 168\n",
      "샘플 단어 시퀀스 평균 길이 : 8.796238649010467\n",
      "BIO 태그 사전 크기 : 10\n",
      "단어 사전 크기 : 17751\n",
      "학습 샘플 시퀀스 형상 :  (49599, 40)\n",
      "학습 샘플 레이블 형상 :  (49599, 40, 10)\n",
      "테스트 샘플 시퀀스 형상 :  (12400, 40)\n",
      "테스트 샘플 레이블 형상 :  (12400, 40, 10)\n",
      "Epoch 1/10\n",
      "388/388 [==============================] - 125s 304ms/step - loss: 0.0257 - accuracy: 0.9668\n",
      "Epoch 2/10\n",
      "388/388 [==============================] - 140s 362ms/step - loss: 0.0085 - accuracy: 0.9873\n",
      "Epoch 3/10\n",
      "388/388 [==============================] - 146s 376ms/step - loss: 0.0056 - accuracy: 0.9914\n",
      "Epoch 4/10\n",
      "388/388 [==============================] - 146s 377ms/step - loss: 0.0045 - accuracy: 0.9930\n",
      "Epoch 5/10\n",
      "388/388 [==============================] - 148s 381ms/step - loss: 0.0038 - accuracy: 0.9941\n",
      "Epoch 6/10\n",
      "388/388 [==============================] - 150s 386ms/step - loss: 0.0033 - accuracy: 0.9947\n",
      "Epoch 7/10\n",
      "388/388 [==============================] - 148s 382ms/step - loss: 0.0030 - accuracy: 0.9953\n",
      "Epoch 8/10\n",
      "388/388 [==============================] - 144s 372ms/step - loss: 0.0028 - accuracy: 0.9955\n",
      "Epoch 9/10\n",
      "388/388 [==============================] - 145s 373ms/step - loss: 0.0025 - accuracy: 0.9959\n",
      "Epoch 10/10\n",
      "388/388 [==============================] - 150s 386ms/step - loss: 0.0025 - accuracy: 0.9960\n",
      "388/388 [==============================] - 20s 50ms/step - loss: 0.0130 - accuracy: 0.9859\n",
      "평가 결과 :  0.9858576655387878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnjnm\\anaconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_OG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gnjnm\\anaconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_DT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gnjnm\\anaconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_TI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gnjnm\\anaconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_LC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gnjnm\\anaconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_FOOD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gnjnm\\anaconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: NNP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gnjnm\\anaconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_PS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NP       1.00      1.00      1.00       303\n",
      "           _       0.53      0.53      0.53       658\n",
      "         _DT       0.99      1.00      1.00     13683\n",
      "       _FOOD       1.00      1.00      1.00     11655\n",
      "         _LC       0.74      0.57      0.65       314\n",
      "         _OG       0.65      0.46      0.54       460\n",
      "         _PS       0.55      0.63      0.59       396\n",
      "         _TI       0.65      0.72      0.68        61\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     27530\n",
      "   macro avg       0.76      0.74      0.75     27530\n",
      "weighted avg       0.97      0.97      0.97     27530\n",
      "\n",
      "F1-score: 96.9%\n"
     ]
    }
   ],
   "source": [
    "# 챗봇 엔진 NER 모델 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from utils.preprocess1 import Preprocess\n",
    "\n",
    "# 학습 파일 불러오기\n",
    "def read_file(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents\n",
    "\n",
    "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dic.bin',\n",
    "               userdic='./utils/user_dic.tsv')\n",
    "\n",
    "# 학습용 말뭉치 데이터를 불러옴\n",
    "corpus = read_file('./models/ner/ner_train.txt')\n",
    "\n",
    "# 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
    "# 단어(w[1]), BIO 태그(w[3]) \n",
    "sentences, tags = [], []\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    for w in t:\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "    \n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "\n",
    "\n",
    "print(\"샘플 크기 : \\n\", len(sentences))\n",
    "print(\"0번 째 샘플 단어 시퀀스 : \\n\", sentences[0])\n",
    "print(\"0번 째 샘플 bio 태그 : \\n\", tags[0])\n",
    "print(\"샘플 단어 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
    "print(\"샘플 단어 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))\n",
    "\n",
    "# 토크나이저 정의\n",
    "# 단어 시퀀스의 경우 preprocess1 객체에서 생성하기 때문에 BIO 태그용 토크나이저 객체만 생성\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False 소문자로 변환하지 않는다.\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "# 단어사전 및 태그 사전 크기 정의\n",
    "# 생성된 사전 리스트 이용해 \n",
    "vocab_size = len(p.word_index) + 1\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
    "print(\"단어 사전 크기 :\", vocab_size)\n",
    "\n",
    "# 학습용 단어 시퀀스 생성\n",
    "# 모델에 입력될 문장의 경우 preprocess1에서 생성한 단어 인덱스 시퀀스를 사용\n",
    "# BIO태그는 위에서 만들어진 사전 데이터를 시퀀스 번호 형태로 인코딩\n",
    "x_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환 하기 위해 사용\n",
    "index_to_ner[0] = 'PAD'\n",
    "\n",
    "# 시퀀스 패딩 처리\n",
    "# 개체명 인식 모델의 입출력 벡터 크기를 동일하게 맞추기 위해 시퀀스 패딩 작업 \n",
    "# 벡터 크기를 위에서 계산한 단어 시퀀스 평균 길이보다 넉넉하게 \n",
    "max_len = 40\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 8:2의 비율로 분리\n",
    "# 8:2 \n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1234)\n",
    "\n",
    "# 출력 데이터를 one-hot encoding\n",
    "# 태그 사전 크기에 맞게 \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
    "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
    "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
    "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
    "\n",
    "\n",
    "# 모델 정의 (Bi-LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 개체 인식 모델 순차 모델 방식 구현\n",
    "# tag_size 만큼의 출력 뉴런에서 제일 확률 높은 출력값 1개를 선택하는 문제라 softmax 활성화 함수 사용 \n",
    "# 손실함수 categorical_crossentropy\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
    "model.save('ner_model.h5')  # 챗봇 엔진의 개체명 인식 모듈에서 사용\n",
    "\n",
    "\n",
    "# 시퀀스를 NER 태그로 변환\n",
    "def sequences_to_tag(sequences):  # 예측값을 index_to_ner를 사용하여 태깅 정보로 변경하는 함수.\n",
    "    result = []\n",
    "    for sequence in sequences:  # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
    "        temp = []\n",
    "        for pred in sequence:  # 시퀀스로부터 예측값을 하나씩 꺼낸다.\n",
    "            pred_index = np.argmax(pred)  # 예를 들어 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
    "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))  # 'PAD'는 'O'로 변경\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "\n",
    "# f1 스코어 계산을 위해 사용\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "# 테스트 데이터셋의 NER 예측\n",
    "# F1 스코어 계산하기 위해 모델의 predict() 함수 통해 테스트용 데이터셋 결과 예측 \n",
    "# 해당 함수 입력은 시퀀스 번호로 인코딩된 테스트용 단어 시퀀스(넘파이 배열) 사용 \n",
    "# 해당 함수 결과는 예측된 NER 태그 정보가 담긴 넘파이 밸열이 반환 됨\n",
    "y_predicted = model.predict(x_test)\n",
    "pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
    "test_tags = sequences_to_tag(y_test)    # 실제 NER\n",
    "\n",
    "# F1 평가 결과\n",
    "# seqeval.metrics 모듈의 classification_report() 함수 통해\n",
    "# NER 태그별 계산된 정밀도, 재현율, F1스코어 출력\n",
    "# f1_score() 함수 통해 f1 스코어 값만 출력 가능 \n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.2 개체명 인식 모듈 생성 \n",
    "\n",
    "- 챗봇 엔진 개체명 인식 모듈 만들기\n",
    "- 개체명 인식 모델 파일 활용해 입력한 문장 내부의 개체명 인식 기능 가짐 \n",
    "- /models/ner.NerModel.py 소스 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "\n",
    "# 개체명 인식 모델 모듈\n",
    "class NerModel:\n",
    "    def __init__(self, model_name, preprocess):\n",
    "\n",
    "        # BIO 태그 클래스 별 레이블\n",
    "        self.index_to_ner = {1: 'O', 2: 'B_DT', 3: 'B_FOOD', 4: 'I', 5: 'B_OG', 6: 'B_PS', 7: 'B_LC', 8: 'NNP', 9: 'B_TI', 0: 'PAD'}\n",
    "\n",
    "        # 의도 분류 모델 불러오기\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        # 챗봇 Preprocess 객체\n",
    "        self.p = preprocess\n",
    "\n",
    "\n",
    "    # 개체명 클래스 예측\n",
    "    def predict(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)\n",
    "\n",
    "        tags = [self.index_to_ner[i] for i in predict_class.numpy()[0]]\n",
    "        return list(zip(keywords, tags))\n",
    "\n",
    "    def predict_tags(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)\n",
    "\n",
    "        tags = []\n",
    "        for tag_idx in predict_class.numpy()[0]:\n",
    "            if tag_idx == 1: continue\n",
    "            tags.append(self.index_to_ner[tag_idx])\n",
    "\n",
    "        if len(tags) == 0: return None\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'B_DT'), ('오전', 'B_DT'), ('13시', 'B_DT'), ('2분', 'B_DT'), ('돈까스', 'B_FOOD'), ('먹', 'O'), ('싶', 'O'), ('주문', 'O'), ('하', 'O'), ('수', 'O'), ('있', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# NerModel 클래스 테스트 코드 \n",
    "# NerModel 객체 생성해 새로운 유형의 문장에서 개체명 인식 \n",
    "# /test.model_ner_test.py\n",
    "\n",
    "from utils.preprocess1 import Preprocess \n",
    "from models.ner.NerModel import NerModel \n",
    "\n",
    "p = Preprocess(word2index_dic = './train_tools/dict/chatbot_dic.bin', \n",
    "               userdic = './utils/user_dic.tsv') \n",
    "\n",
    "ner = NerModel(model_name='./models/ner/ner_model.h5', preprocess=p) \n",
    "query = '오늘 오전 13시 2분에 돈까스 먹고 싶은데 주문 할 수 있나요?' \n",
    "predicts = ner.predict(query) \n",
    "print(predicts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 답변 검색 \n",
    "\n",
    "- 화자로부터 입력된 문장이 전처리, 의도 분류, 개체명 인식 과정을 거쳐 해석된 데이터 기반으로 적절한 답변을 학습 DB로 부터 검색하는 방법\n",
    "- 챗봇 엔진이 자연어 처리 통해 해석한 문장을 토대로 유사한 답변을 검색하는일은 중요 \n",
    "- SQL 구문 이용 룰 베이스 기반으로 답변을 검색하는 방법 소개\n",
    "- 입력 되는 문장 해석 = 딥러닝, 해석 결과 기반 답변 찾는 과정 = 룰 베이스 기반 \n",
    "\n",
    "### 8.7.1 데이터베이스 제어 모듈 생성 \n",
    "\n",
    "- 데이터 베이스 제어를 쉽게 할 수 있는 모듈 만들기 \n",
    "- /utils.Database.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스 제어 모듈\n",
    "\n",
    "import pymysql\n",
    "import pymysql.cursors\n",
    "import logging\n",
    "\n",
    "\n",
    "class Database:\n",
    "    '''\n",
    "    database 제어\n",
    "    '''\n",
    "\n",
    "    def __init__(self, host, user, password, db_name, charset='utf8'):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.charset = charset\n",
    "        self.db_name = db_name\n",
    "        self.conn = None\n",
    "\n",
    "    # DB 연결\n",
    "    def connect(self):\n",
    "        if self.conn != None:\n",
    "            return\n",
    "\n",
    "        self.conn = pymysql.connect(\n",
    "            host=self.host,\n",
    "            user=self.user,\n",
    "            password=self.password,\n",
    "            db=self.db_name,\n",
    "            charset=self.charset\n",
    "        )\n",
    "\n",
    "    # DB 연결 닫기\n",
    "    def close(self):\n",
    "        if self.conn is None:\n",
    "            return\n",
    "\n",
    "        if not self.conn.open:\n",
    "            self.conn = None\n",
    "            return\n",
    "        self.conn.close()\n",
    "        self.conn = None\n",
    "\n",
    "    # SQL 구문 실행\n",
    "    def execute(self, sql):\n",
    "        last_row_id = -1\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                cur.execute(sql)\n",
    "            self.conn.commit()\n",
    "            last_row_id = cur.lastrowid\n",
    "            # logging.debug(\"excute last_row_id : %d\", last_row_id)\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "\n",
    "        finally:\n",
    "            return last_row_id\n",
    "\n",
    "    # SELECT 구문 실행 후, 단 1개의 데이터 ROW만 불러옴\n",
    "    def select_one(self, sql):\n",
    "        result = None\n",
    "\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cur:\n",
    "                cur.execute(sql)\n",
    "                result = cur.fetchone()\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "\n",
    "        finally:\n",
    "            return result\n",
    "\n",
    "    # SELECT 구문 실행 후, 전체 데이터 ROW만 불러옴\n",
    "    def select_all(self, sql):\n",
    "        result = None\n",
    "\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cur:\n",
    "                cur.execute(sql)\n",
    "                result = cur.fetchall()\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "\n",
    "        finally:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.2 답변 검색 모듈 생성 \n",
    "\n",
    "- 입력되는 문장을 전처리, 의도 분류, 개체명 인식 과정 거쳐 나온 자연어 해석 결과를 이용해 학습 DB 에서 적적한 답변을 검색 \n",
    "- 해석된 결과 항목에 따라 학습 DB 에서 어떤 방식으로 답변을 검색할지 결정하는 일은 챗봇 엔진 설계자 몫 \n",
    "- 의도명과 개체명 2가지 항목으로만 답변 검색 \n",
    "- /utils.FindAnswer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 답변 검색 모듈 \n",
    "\n",
    "class FinAnswer:\n",
    "    # FindAnswer 클래스 생성자.  - Database 인스턴스 객체 인자로 받아 클래스 멤버 변수로 저장. \n",
    "    # 이 객체로 답변 검색 \n",
    "    def __init__(self, db):\n",
    "        self.db = db \n",
    "\n",
    "    # 검색 쿼리 생성\n",
    "    # 의도명만 검색할지, 여러 종류 개체명 태그와 함께 검색할지 결정하는 조건 \n",
    "    def _make_query(self, intent_name, ner_tags):\n",
    "        sql = \"select * from 'chatbot_train_data'\"\n",
    "        if intent_name != None and ner_tags == None:\n",
    "            sql = sql + \" where intent={} \".format(intent_name)\n",
    "\n",
    "        elif intent_name != None and ner_tags != None:\n",
    "            where = \" where intent=%s \" % intent_name\n",
    "            if len(ner_tags) > 0:\n",
    "                where += 'and (' \n",
    "                for ne in ner_tags: \n",
    "                    where += \" ner like '%{}%' or \".format(ne)\n",
    "                where = where[:-3] + ')'\n",
    "            sql = sql + where\n",
    "\n",
    "        # 동일한 답변이 2개 이상인 경우, 랜덤으로 선택\n",
    "        sql = sql + ' order by rand() limit 1'\n",
    "        return sql \n",
    "\n",
    "    # 답변 검색 \n",
    "    # 의도명(intent_name) 개체명 태그 리스트(ner_tags) 이용해 질문 답변 검색하는 메서드 \n",
    "    # 인자로 제공된 2가지(의도명, 개체명 태그 리스트) 검색시 실패할 수 있음\n",
    "    # 이런 경우 의도명만 이용해 답변 검색 \n",
    "    # 정확한 조건 답변 없으면 차선책으로 동일 의도 가지는 답변만 검색 \n",
    "    def search(self, intent_name, ner_tags): \n",
    "        # 의도명과 개체명으로 답변 검색 \n",
    "        sql = self._make_query(intent_name, ner_tags) \n",
    "        answer = self.db.select_one(sql) \n",
    "\n",
    "        # 검색되는 답변 없으면 의도명만 검색 \n",
    "        if answer is None: \n",
    "            sql = self._make_query(intent_name, None) \n",
    "            answer = self.db.select_one(sql) \n",
    "\n",
    "        return (answer['answer'], answer['answer_image']) \n",
    "\n",
    "    # NER 태그를 실제 입력한 단어로 변환 \n",
    "    # 예: '자장면 주문할게요' 텍스트 챗봇 엔진 입력 \n",
    "    # 챗봇엔진은 자장면 = B_FOOD 개체명 인식 \n",
    "    # 검색된 답변이 '{B_FOOD} 주문 처리 완료 되었습니다. 주문해 주셔서 감사합니다' \n",
    "    # 답변 내용 속 {B_FOOD} 자장면으로 변환해 주는 함수 \n",
    "    def tag_to_word(self, ner_predicts, answer): \n",
    "        for word, tag in ner_predicts:\n",
    "\n",
    "            # 변환해야 하는 태그가 있는 경우 추가 \n",
    "            if tag == 'B_FOOD': \n",
    "                answer = answer.replace(tag, word) \n",
    "\n",
    "        answer = answer.replace('}', '') \n",
    "        answer = answer.replace('}', '') \n",
    "        return answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:(1064, 'You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near \\'from chatbot_train_data where intent = \"주문\" and ( ner like \\'%B_FOOD%\\' ) othe\\' at line 1')\n",
      "ERROR:root:(1064, \"You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'from chatbot_train_data where intent = '주문'  other by rand() limit 1' at line 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 :  자장면 주문할게요\n",
      "====================================================================================================\n",
      "의도 파악 :  주문\n",
      "개체명 인식 :  [('자장면', 'B_FOOD'), ('주문', 'O')]\n",
      "답변 검색에 필요한 NER 태그 :  ['B_FOOD']\n",
      "====================================================================================================\n",
      "답변 :  죄송해요 무슨 말인지 모르겠어요\n"
     ]
    }
   ],
   "source": [
    "# FindAnswer 클래스 테스트 코딩 \n",
    "# 챗봇 엔진 전체 동작 과정 보여줌 - 챗봇 엔진 동작 코드 \n",
    "from config.DBconfig import *\n",
    "from utils.Database import Database\n",
    "from utils.preprocess1 import Preprocess\n",
    "\n",
    "# 전처리 객체 생성\n",
    "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dic.bin',\n",
    "               userdic='./utils/user_dic.tsv')\n",
    "\n",
    "# 질문/답변 학습 디비 연결 객체 생성\n",
    "db = Database(\n",
    "    host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME\n",
    ")\n",
    "db.connect()    # 디비 연결\n",
    "\n",
    "# 원문\n",
    "# query = \"오전에 탕수육 10개 주문합니다\"\n",
    "# query = \"화자의 질문 의도를 파악합니다.\"\n",
    "# query = \"안녕하세요\"\n",
    "query = \"자장면 주문할게요\"\n",
    "\n",
    "# 의도 파악\n",
    "from models.intent.IntentModel import IntentModel\n",
    "intent = IntentModel(model_name='./models/intent/intent_model.h5', preprocess=p)\n",
    "predict = intent.predict_class(query)\n",
    "intent_name = intent.labels[predict]\n",
    "\n",
    "# 개체명 인식\n",
    "from models.ner.NerModel import NerModel\n",
    "ner = NerModel(model_name='./models/ner/ner_model.h5', preprocess=p)\n",
    "predicts = ner.predict(query)\n",
    "ner_tags = ner.predict_tags(query)\n",
    "\n",
    "print(\"질문 : \", query)\n",
    "print(\"=\" * 100)\n",
    "print(\"의도 파악 : \", intent_name)\n",
    "print(\"개체명 인식 : \", predicts)\n",
    "print(\"답변 검색에 필요한 NER 태그 : \", ner_tags)\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 답변 검색\n",
    "# 답변 존재하지 않는 경우 예외 발생. 해당 문장 이해할 수 없다는 문장 출력 추가 \n",
    "# 예외 사항이 발생하는 질문 내용들을 모델 학습 데이터로 활용\n",
    "from utils.FindAnswer import FindAnswer\n",
    "\n",
    "try:\n",
    "    f = FindAnswer(db)\n",
    "    answer_text, answer_image = f.search(intent_name, ner_tags)\n",
    "    answer = f.tag_to_word(predicts, answer_text)\n",
    "except:\n",
    "    answer = \"죄송해요 무슨 말인지 모르겠어요\"\n",
    "\n",
    "print(\"답변 : \", answer)\n",
    "\n",
    "db.close() # 디비 연결 끊음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b6f0d8d647a14ce7aec9650edf6792f5fc61b159b6795cf61c93c7b5c41aef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
